{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Configure notebook and download index files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 468 µs (started: 2022-02-09 23:26:47 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# To have time runtime for cells\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting index file for year: 2021 remote=https://s3.amazonaws.com/irs-form-990/index_2021.csv local=/opt/conda/miniconda3/lib/python3.8/site-packages/irsx/CSV/index_2021.csv\n",
      "Beginning streaming download of https://s3.amazonaws.com/irs-form-990/index_2021.csv\n",
      "Total file size: 55.76 MB\n",
      "Download completed to /opt/conda/miniconda3/lib/python3.8/site-packages/irsx/CSV/index_2021.csv in 0:00:01.413513\n",
      "time: 2.13 s (started: 2022-02-09 23:26:47 +00:00)\n"
     ]
    }
   ],
   "source": [
    "#Download Index file which contains the object_id which are required to download the IRS files\n",
    "!irsx_index --year=2021 --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing filing 202012339349300801\n",
      "Printing result to file temp\n",
      "File already available at /opt/conda/miniconda3/lib/python3.8/site-packages/irsx/XML/202012339349300801_public.xml -- skipping\n",
      "Filing 202012339349300801 is version 2019v5.1\n",
      "In 202012339349300801 keyerrors: [{'schedule_name': 'IRS990', 'group_keyerrors': [], 'keyerrors': [{'element_path': '/IRS990/DonorRstrOrQuasiEndowmentsInd'}, {'element_path': '/IRS990/BusinessRlnWith35CtrlEntInd'}, {'element_path': '/IRS990/OrganizationFollowsFASB117Ind'}, {'element_path': '/IRS990/NoDonorRestrictionNetAssetsGrp/BOYAmt'}, {'element_path': '/IRS990/NoDonorRestrictionNetAssetsGrp/EOYAmt'}]}]\n",
      "\n",
      "\n",
      "time: 644 ms (started: 2022-02-09 23:26:50 +00:00)\n"
     ]
    }
   ],
   "source": [
    "!irsx --file temp --verbose 202012339349300801"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "Runterladen der einzelnen XML Files mit Spark, auslesen der wichtigen Daten und schreiben in ein Dateiformat, welches besser für die Analyse geeignet ist.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 809 µs (started: 2022-02-09 23:26:50 +00:00)\n"
     ]
    }
   ],
   "source": [
    "#Set path of index file, and amount of lines to be read (amount of IRS files later processed)\n",
    "path = \"/opt/conda/miniconda3/lib/python3.8/site-packages/irsx/CSV/index_2021.csv\"\n",
    "nrows = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETURN_ID  FILING_TYPE  EIN        TAX_PERIOD  SUB_DATE               TAXPAYER_NAME                                        RETURN_TYPE  DLN             OBJECT_ID\n",
      "17606342   EFILE        452772761  201906      1/21/2021 10:02:51 AM  CAMDENS CHARTER SCHOOL NETWORK INC                   990          93493065013010  202010659349301301  \n",
      "17606343   EFILE        237061115  201906      1/21/2021 10:02:51 AM  JACKSON STATE UNIVERSITY DEVELOPMENT FOUNDATION INC  990          93493072000410  202010729349300041  \n",
      "17606347   EFILE        344427516  201904      1/21/2021 10:02:52 AM  TIFFIN UNIVERSITY                                    990          93493072000210  202010729349300021  \n",
      "17606350   EFILE        840865247  201912      1/21/2021 10:03:29 AM  NETWORK MINISTRIES INC                               990          93493072008360  202010729349300836  \n",
      "time: 157 ms (started: 2022-02-09 23:26:50 +00:00)\n"
     ]
    }
   ],
   "source": [
    "#Have a look at the index file to get an idea, we need the last column (zoom out to see whole file)\n",
    "!head -5 {path} | column -t -s,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.79 s (started: 2022-02-09 23:26:50 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Read indexfile as PandasDF, extract object_ids and convert to PysparkDF\n",
    "import pandas as pd\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df21 = pd.read_csv(path, index_col=False, dtype=str, nrows= nrows) # read all as string, not beautiful but we only need object id anyways\n",
    "\n",
    "sdf = spark.createDataFrame(df21[\"OBJECT_ID\"], StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 39.1 ms (started: 2022-02-09 23:26:53 +00:00)\n"
     ]
    }
   ],
   "source": [
    "#Define custom transformation which downloads the IRS files via IRSX, transforms it into a python dict and extracts the required fields\n",
    "from irsx.xmlrunner import XMLRunner\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "xml_runner = XMLRunner()\n",
    "def transform_data(object_id):\n",
    "    try: #Download files with basic error handling\n",
    "        filing = xml_runner.run_filing(object_id)\n",
    "        schedules = filing.list_schedules()\n",
    "    except: \n",
    "        print(f\"Transform error for id {object_id}\")\n",
    "        return [\"\", \"\", \"\", \"\", 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    \n",
    "    # Initialize with values to avoid NANs \n",
    "    ein, state, name = \"\", \"\", \"\"\n",
    "    state, name, revenue, revenueEZ, vol_cnt, empl_cnt, rvn_ls_exp, liab_eoy, liab_boy,assts_eoy, assts_boy = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    #For different IRS forms the data is in different places, get the required \n",
    "    if \"ReturnHeader990x\" in schedules:\n",
    "        header = filing.get_parsed_sked(\"ReturnHeader990x\")\n",
    "        header_part_i = header[0][\"schedule_parts\"][\"returnheader990x_part_i\"]\n",
    "        ein = header_part_i[\"ein\"]\n",
    "        state = header_part_i.get(\"USAddrss_SttAbbrvtnCd\", \"XX\")\n",
    "        name = header_part_i[\"BsnssNm_BsnssNmLn1Txt\"]\n",
    "        \n",
    "    if \"IRS990EZ\" in schedules:\n",
    "        irs990ez = filing.get_parsed_sked(\"IRS990EZ\")\n",
    "        irs990ez_part_i = irs990ez[0][\"schedule_parts\"].get(\"ez_part_i\", None)\n",
    "        if irs990ez_part_i:\n",
    "            revenueEZ = irs990ez_part_i.get(\"TtlRvnAmt\", 0)        \n",
    "    \n",
    "    if \"IRS990\" in schedules:\n",
    "        irs990 = filing.get_parsed_sked(\"IRS990\")\n",
    "        irs990_part_i = irs990[0][\"schedule_parts\"][\"part_i\"]\n",
    "        revenue = irs990_part_i[\"CYTtlRvnAmt\"]\n",
    "        vol_cnt = int(irs990_part_i.get(\"TtlVlntrsCnt\", 0))\n",
    "        empl_cnt = int(irs990_part_i.get(\"TtlEmplyCnt\", 0))\n",
    "        rvn_ls_exp = int(irs990_part_i.get(\"CYRvnsLssExpnssAmt\", 0))\n",
    "        liab_eoy = int(irs990_part_i.get(\"TtlLbltsEOYAmt\", 0))\n",
    "        liab_boy = int(irs990_part_i.get(\"TtlLbltsBOYAmt\", 0))\n",
    "        assts_eoy = int(irs990_part_i.get(\"TtlAsstsEOYAmt\", 0))\n",
    "        assts_boy = int(irs990_part_i.get(\"TtlAsstsBOYAmt\", 0))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    revenue = int(revenue) + int(revenueEZ) #Sum the two rev values because we want to only return one\n",
    "    return [object_id, ein, state, name, revenue, vol_cnt, empl_cnt, rvn_ls_exp, liab_eoy, liab_boy,assts_eoy, assts_boy ]\n",
    "     \n",
    "#Define PySpark Schema    \n",
    "my_schema = StructType([\n",
    "    StructField(\"ObjectID\", StringType(), nullable=False),\n",
    "    StructField(\"EIN\", StringType(), nullable=False),\n",
    "    StructField(\"State\", StringType(), nullable=False),\n",
    "    StructField(\"Name\", StringType(), nullable=False),\n",
    "    StructField(\"Revenue\", IntegerType(), nullable=False),\n",
    "    StructField(\"TtlVlntrsCnt\", IntegerType(), nullable=False),\n",
    "    StructField(\"TtlEmplyCnt\", IntegerType(), nullable=False),\n",
    "    StructField(\"CYRvnsLssExpnssAmt\", IntegerType(), nullable=False),\n",
    "    StructField(\"TtlLbltsEOYAmt\", IntegerType(), nullable=False),\n",
    "    StructField(\"TtlLbltsBOYAmt\", IntegerType(), nullable=False),\n",
    "    StructField(\"TtlAsstsEOYAmt\", IntegerType(), nullable=False),\n",
    "    StructField(\"TtlAsstsBOYAmt\", IntegerType(), nullable=False),\n",
    "])\n",
    "\n",
    "spark_transform_data = udf(lambda z: transform_data(z), my_schema)\n",
    "#spark.udf.register(\"spark_transform_data\", spark_transform_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['202021359349301552',\n",
       " '541965765',\n",
       " 'VA',\n",
       " 'PARK PLACE SCHOOL INC',\n",
       " 671334,\n",
       " 56,\n",
       " 19,\n",
       " -14709,\n",
       " 6015,\n",
       " 14414,\n",
       " 725910,\n",
       " 750315]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 65.9 ms (started: 2022-02-09 23:26:53 +00:00)\n"
     ]
    }
   ],
   "source": [
    "#Basic example of what the function return\n",
    "transform_data(\"202021359349301552\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "== Physical Plan ==\n",
      "*(2) Project [pythonUDF0#36.ObjectID AS ObjectID#12, pythonUDF0#36.EIN AS EIN#13, pythonUDF0#36.State AS State#14, pythonUDF0#36.Name AS Name#15, pythonUDF0#36.Revenue AS Revenue#16, pythonUDF0#36.TtlVlntrsCnt AS TtlVlntrsCnt#17, pythonUDF0#36.TtlEmplyCnt AS TtlEmplyCnt#18, pythonUDF0#36.CYRvnsLssExpnssAmt AS CYRvnsLssExpnssAmt#19, pythonUDF0#36.TtlLbltsEOYAmt AS TtlLbltsEOYAmt#20, pythonUDF0#36.TtlLbltsBOYAmt AS TtlLbltsBOYAmt#21, pythonUDF0#36.TtlAsstsEOYAmt AS TtlAsstsEOYAmt#22, pythonUDF0#36.TtlAsstsBOYAmt AS TtlAsstsBOYAmt#23]\n",
      "+- BatchEvalPython [<lambda>(value#0)], [pythonUDF0#36]\n",
      "   +- *(1) Scan ExistingRDD[value#0]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "# Apply the transformation function and write the result as csv and parquet to HDFS\n",
    "anz = sdf.count()\n",
    "print(anz)\n",
    "sdf2 = sdf.withColumn('valuelist', spark_transform_data('value')).select(\"valuelist.*\")\n",
    "sdf2.explain()\n",
    "sdf2.write.mode('overwrite').csv(f\"hdfs://big-spark-cluster-m/user/root/{anz}ps.csv\")\n",
    "sdf2.write.mode('overwrite').save(f\"hdfs://big-spark-cluster-m/user/root/{anz}ps.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeiten von erfolgreichen Läufen (u.a. aufgrund von Caching nicht sinnvoll vergleichbar)\n",
    "\n",
    "|Anzahl | Zeit| Kommentar |  \n",
    "-------|--------|---------------\n",
    "|4710 | time: 2min 44s (started: 2022-01-17 18:46:15 +00:00) | erster Versuch (zusätzlicher Zeitbedarf für Sampling  & lokal)|\n",
    "|46099| time: 12min 22s (started: 2022-01-17 20:42:31 +00:00) | erster Versuch mit ErrorHandling (zusätzlicher Zeitbedarf für Sampling & lokal)|\n",
    "|184781| time: 1h 14min 7s (started: 2022-01-25 12:37:15 +00:00) | -  (zusätzlicher Zeitbedarf für Sampling & lokal)|\n",
    "|184781| time: 14min 23s (started: 2022-02-02 11:34:34 +00:00) | mehr Attribute, dafür kein Samplen mehr sonder direkt beim Einlesen, speichern in HDFS|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "In diesem Schritt wird das gerade erzeugte File wieder eingelesen, um Analysen zu ermöglichen.  \n",
    "Hier ist die Verwendung von Pyspark nicht mehr zwangsläufig erforderlich, wird aber für unser einfaches Beispiel trotzdem verwendet.  \n",
    "Wenn Pyspark verwendet wird, sollte das File auf HDFS liegen, damit alle Worker Zugriff darauf haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ObjectID: string (nullable = true)\n",
      " |-- EIN: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Revenue: integer (nullable = true)\n",
      " |-- TtlVlntrsCnt: integer (nullable = true)\n",
      " |-- TtlEmplyCnt: integer (nullable = true)\n",
      " |-- CYRvnsLssExpnssAmt: integer (nullable = true)\n",
      " |-- TtlLbltsEOYAmt: integer (nullable = true)\n",
      " |-- TtlLbltsBOYAmt: integer (nullable = true)\n",
      " |-- TtlAsstsEOYAmt: integer (nullable = true)\n",
      " |-- TtlAsstsBOYAmt: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(pyspark.sql.dataframe.DataFrame,\n",
       " None,\n",
       " [Row(ObjectID='202010659349301301', EIN='452772761', State='NJ', Name=\"CAMDEN'S CHARTER SCHOOL\", Revenue=5148336, TtlVlntrsCnt=0, TtlEmplyCnt=6, CYRvnsLssExpnssAmt=1302111, TtlLbltsEOYAmt=15896401, TtlLbltsBOYAmt=17300598, TtlAsstsEOYAmt=24491485, TtlAsstsBOYAmt=24842081),\n",
       "  Row(ObjectID='202010729349300041', EIN='237061115', State='MS', Name='JACKSON STATE UNIVERSITY', Revenue=10386900, TtlVlntrsCnt=0, TtlEmplyCnt=0, CYRvnsLssExpnssAmt=3123744, TtlLbltsEOYAmt=22864113, TtlLbltsBOYAmt=20379310, TtlAsstsEOYAmt=60511849, TtlAsstsBOYAmt=54569751),\n",
       "  Row(ObjectID='202010729349300021', EIN='344427516', State='OH', Name='TIFFIN UNIVERSITY', Revenue=64789812, TtlVlntrsCnt=16, TtlEmplyCnt=1194, CYRvnsLssExpnssAmt=995667, TtlLbltsEOYAmt=28335011, TtlLbltsBOYAmt=30287043, TtlAsstsEOYAmt=84770678, TtlAsstsBOYAmt=85702921),\n",
       "  Row(ObjectID='202010729349300836', EIN='840865247', State='CO', Name='NETWORK MINISTRIES INC', Revenue=218029, TtlVlntrsCnt=0, TtlEmplyCnt=0, CYRvnsLssExpnssAmt=63859, TtlLbltsEOYAmt=3000, TtlLbltsBOYAmt=2507, TtlAsstsEOYAmt=240903, TtlAsstsBOYAmt=176551),\n",
       "  Row(ObjectID='202000669349300135', EIN='205647589', State='SC', Name='FAMILY PROMISE OF BEAUFORT COUNTY', Revenue=482668, TtlVlntrsCnt=800, TtlEmplyCnt=7, CYRvnsLssExpnssAmt=84363, TtlLbltsEOYAmt=12418, TtlLbltsBOYAmt=13837, TtlAsstsEOYAmt=715271, TtlAsstsBOYAmt=632327)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Einlesen des Files von HDFS als PysparkDF\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark2 = SparkSession.builder.appName(\"Test1\").getOrCreate()\n",
    "\n",
    "hdfs_path = \"hdfs://big-spark-cluster-m/user/root/184781.parquet\"\n",
    "df = spark2.read.parquet(hdfs_path, header=True, inferSchema=True)\n",
    "\n",
    "type(df), df.printSchema(), df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -head {hdfs_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umwandlung in RDD und dann Aufsummierung der Revenues staatenweise\n",
    "from operator import add\n",
    "\n",
    "two_col_df = df.select(\"State\", \"Revenue\") #Reduzierung auf die notwendigen Spalten \n",
    "two_col_df.printSchema()\n",
    "rdd = two_col_df.rdd #Umwandlung in ein Key, Value RDD\n",
    "\n",
    "reduced_rdd = rdd.reduceByKey(add).sortBy(lambda x: x[1], ascending = False) #Aggreation und Sortierung nach Rev\n",
    "state_rev = reduced_rdd.collectAsMap() #Umwandlung in Python Dict\n",
    "state_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import rgb2hex, Normalize\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib import cm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1], projection=ccrs.LambertConformal())\n",
    "\n",
    "ax.set_extent([-125, -66.5, 20, 50], ccrs.Geodetic())\n",
    "\n",
    "shapename = 'admin_1_states_provinces_lakes'\n",
    "states_shp = shpreader.natural_earth(resolution='110m',\n",
    "                                     category='cultural', name=shapename)\n",
    "ax.background_patch.set_visible(False)\n",
    "ax.outline_patch.set_visible(False)\n",
    "\n",
    "ax.set_title('Revenue summed per state')\n",
    "\n",
    "cmap = cm.get_cmap(\"YlGn\")\n",
    "min_val = min(state_rev.values())\n",
    "max_val = max(state_rev.values())\n",
    "\n",
    "for astate in shpreader.Reader(states_shp).records():\n",
    "\n",
    "    edgecolor = 'black'\n",
    "\n",
    "    try: #Set values of the states depending on the values from our data\n",
    "        rev_one_state = state_rev[ astate.attributes['postal'] ]\n",
    "    except:\n",
    "        rev_one_state = 0\n",
    "\n",
    "    # Add shape with corresponding color\n",
    "    ax.add_geometries([astate.geometry], ccrs.PlateCarree(),\n",
    "                      facecolor=cmap(np.sqrt((rev_one_state-min_val)/(max_val-min_val)))[:3], edgecolor=edgecolor)\n",
    "\n",
    "# Plot Colorbarlegend\n",
    "ax_c = fig.add_axes([0.9, 0.1, 0.03, 0.8])\n",
    "norm = Normalize(vmin=min_val, vmax=max_val)\n",
    "cb = ColorbarBase(ax_c,cmap=cmap,norm=norm,orientation='vertical',\n",
    "                  label='revenue')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
