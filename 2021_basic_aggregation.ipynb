{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RETURN_ID</th>\n",
       "      <th>FILING_TYPE</th>\n",
       "      <th>EIN</th>\n",
       "      <th>TAX_PERIOD</th>\n",
       "      <th>SUB_DATE</th>\n",
       "      <th>TAXPAYER_NAME</th>\n",
       "      <th>RETURN_TYPE</th>\n",
       "      <th>DLN</th>\n",
       "      <th>OBJECT_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17606342</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>452772761</td>\n",
       "      <td>201906</td>\n",
       "      <td>1/21/2021 10:02:51 AM</td>\n",
       "      <td>CAMDENS CHARTER SCHOOL NETWORK INC</td>\n",
       "      <td>990</td>\n",
       "      <td>93493065013010</td>\n",
       "      <td>202010659349301301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17606343</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>237061115</td>\n",
       "      <td>201906</td>\n",
       "      <td>1/21/2021 10:02:51 AM</td>\n",
       "      <td>JACKSON STATE UNIVERSITY DEVELOPMENT FOUNDATIO...</td>\n",
       "      <td>990</td>\n",
       "      <td>93493072000410</td>\n",
       "      <td>202010729349300041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17606347</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>344427516</td>\n",
       "      <td>201904</td>\n",
       "      <td>1/21/2021 10:02:52 AM</td>\n",
       "      <td>TIFFIN UNIVERSITY</td>\n",
       "      <td>990</td>\n",
       "      <td>93493072000210</td>\n",
       "      <td>202010729349300021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17606350</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>840865247</td>\n",
       "      <td>201912</td>\n",
       "      <td>1/21/2021 10:03:29 AM</td>\n",
       "      <td>NETWORK MINISTRIES INC</td>\n",
       "      <td>990</td>\n",
       "      <td>93493072008360</td>\n",
       "      <td>202010729349300836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17606351</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>205647589</td>\n",
       "      <td>201912</td>\n",
       "      <td>1/21/2021 10:03:29 AM</td>\n",
       "      <td>FAMILY PROMISE OF BEAUFORT COUNTY</td>\n",
       "      <td>990</td>\n",
       "      <td>93493066001350</td>\n",
       "      <td>202000669349300135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RETURN_ID FILING_TYPE        EIN TAX_PERIOD               SUB_DATE  \\\n",
       "0  17606342       EFILE  452772761     201906  1/21/2021 10:02:51 AM   \n",
       "1  17606343       EFILE  237061115     201906  1/21/2021 10:02:51 AM   \n",
       "2  17606347       EFILE  344427516     201904  1/21/2021 10:02:52 AM   \n",
       "3  17606350       EFILE  840865247     201912  1/21/2021 10:03:29 AM   \n",
       "4  17606351       EFILE  205647589     201912  1/21/2021 10:03:29 AM   \n",
       "\n",
       "                                       TAXPAYER_NAME RETURN_TYPE  \\\n",
       "0                 CAMDENS CHARTER SCHOOL NETWORK INC         990   \n",
       "1  JACKSON STATE UNIVERSITY DEVELOPMENT FOUNDATIO...         990   \n",
       "2                                  TIFFIN UNIVERSITY         990   \n",
       "3                             NETWORK MINISTRIES INC         990   \n",
       "4                  FAMILY PROMISE OF BEAUFORT COUNTY         990   \n",
       "\n",
       "              DLN           OBJECT_ID  \n",
       "0  93493065013010  202010659349301301  \n",
       "1  93493072000410  202010729349300041  \n",
       "2  93493072000210  202010729349300021  \n",
       "3  93493072008360  202010729349300836  \n",
       "4  93493066001350  202000669349300135  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "path = \"/opt/conda/miniconda3/lib/python3.8/site-packages/irsx/CSV/index_2021.csv\"\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "df21 = pd.read_csv(path, index_col=False, dtype=str) # read all as string, not beautiful but we only need object id anyways\n",
    "df21.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "sdf = spark.createDataFrame(df21[\"OBJECT_ID\"], StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(z)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from irsx.xmlrunner import XMLRunner\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "xml_runner = XMLRunner()\n",
    "def transform_data(e):\n",
    "    filing = xml_runner.run_filing(e)\n",
    "    schedules = filing.list_schedules()\n",
    "    \n",
    "    ein = 0\n",
    "    state = 0\n",
    "    name = 0\n",
    "    revenue = 0\n",
    "    revenueEZ = 0\n",
    "    \n",
    "    if \"ReturnHeader990x\" in schedules:\n",
    "        header = filing.get_parsed_sked(\"ReturnHeader990x\")\n",
    "        header_part_i = header[0][\"schedule_parts\"][\"returnheader990x_part_i\"]\n",
    "        ein = header_part_i[\"ein\"]\n",
    "        try:\n",
    "            state = header_part_i[\"USAddrss_SttAbbrvtnCd\"]\n",
    "        except KeyError:\n",
    "            state = XX\n",
    "        name = header_part_i[\"BsnssNm_BsnssNmLn1Txt\"]\n",
    "        \n",
    "    if \"IRS990EZ\" in schedules:\n",
    "        irs990ez = filing.get_parsed_sked(\"IRS990EZ\")\n",
    "        irs990ez_part_i = irs990ez[0][\"schedule_parts\"][\"ez_part_i\"]\n",
    "        revenueEZ = irs990ez_part_i[\"TtlRvnAmt\"]        \n",
    "    \n",
    "    if \"IRS990\" in schedules:\n",
    "        irs990 = filing.get_parsed_sked(\"IRS990\")\n",
    "        irs990_part_i = irs990[0][\"schedule_parts\"][\"part_i\"]\n",
    "        revenue = irs990_part_i[\"CYTtlRvnAmt\"]\n",
    "    \n",
    "    revenue = int(revenue) + int(revenueEZ)\n",
    "    return [e, ein, state, name, revenue]\n",
    "     \n",
    "    \n",
    "my_schema = StructType([\n",
    "    StructField(\"ObjectID\", StringType(), nullable=False),\n",
    "    StructField(\"EIN\", StringType(), nullable=False),\n",
    "    StructField(\"State\", StringType(), nullable=False),\n",
    "    StructField(\"Name\", StringType(), nullable=False),\n",
    "    StructField(\"Revenue\", IntegerType(), nullable=False),\n",
    "])\n",
    "\n",
    "spark_transform_data = udf(lambda z: transform_data(z), my_schema)\n",
    "spark.udf.register(\"spark_transform_data\", spark_transform_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/14 11:18:02 WARN org.apache.spark.scheduler.TaskSetManager: Stage 0 contains a task of very large size (10391 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 4:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-----+--------------------+--------+\n",
      "|          ObjectID|      EIN|State|                Name| Revenue|\n",
      "+------------------+---------+-----+--------------------+--------+\n",
      "|202120119349100127|566036060|   NV|EMMA MILLER CRUTE...|       0|\n",
      "|202021969349305247|850365466|   NM|FARMINGTON PUBLIC...|   55568|\n",
      "|202023049349300237|237108470|   MN|SOUTHWEST MINNESO...| 4232311|\n",
      "|202021959349100107|471578294|   IA|CENTRAL STATES ST...|       0|\n",
      "|202013189349308441|262465531|   WA|NEW FAMILY TRADIT...|  573523|\n",
      "|202012659349300621|952232881|   CA|    MATURANGO MUSEUM|  381985|\n",
      "|202042869349301609|251740106|   PA|TURKEYTOWN SOUTH ...|  349390|\n",
      "|202003159349300515|510209843|   DE|THE MINISTRY OF C...|11446827|\n",
      "|202032869349301538|352550450|   IN|ALPHA XI DELTA FR...|  381964|\n",
      "|202043219349208074|592737710|   TN|GOLD COAST PROMOT...|   85793|\n",
      "|202033429349300618|741690314|   CA|PINE MOUNTAIN CLU...| 5558885|\n",
      "|202041969349202244|420672550|   IA|VETERANS OF FOREI...|   10776|\n",
      "|202033039349302033|208817225|   MD|The AOPA Foundati...| 9225285|\n",
      "|202013199349100621|956029752|   CA| DR SEUSS FOUNDATION|       0|\n",
      "|202131329349201558|043046769|   MA|NEWBURY STREET LE...|   59600|\n",
      "|202043219349319304|550700239|   WV|PUBLIC DEFENDER C...|  642241|\n",
      "|202120679349200512|150502381|   NY|United Steelworke...|   98020|\n",
      "|202023389349200602|133458345|   NY|FRIENDS OF-OR-ISR...|   74212|\n",
      "|202003159349101850|352435568|   MA|THE ENDURANCE FOU...|       0|\n",
      "|202011989349300641|820907092|   NJ|GAN HENEL EARLY L...|  670801|\n",
      "+------------------+---------+-----+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "small_sdf = sdf.sample(0.0002).repartition(10) #get 72 entries\n",
    "small_sdf2 = small_sdf.withColumn('valuelist', spark_transform_data('value')).select(\"valuelist.*\")\n",
    "small_sdf2.show()\n",
    "#TODO write somewhere measure time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a Sparkession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=SparkByExamples.com, master=local[1]) created by getOrCreate at /tmp/ipykernel_2197/4270203480.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2197/375114469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BigDataIRS3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    343\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=SparkByExamples.com, master=local[1]) created by getOrCreate at /tmp/ipykernel_2197/4270203480.py:5 "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName=\"BigDataIRS3\") #TODO use getOrCreate\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen des Files  \n",
    "Das File kann entweder lokal oder mittels hdfs eingelesen werden  \n",
    "Falls das File lokal eingelesen wird muss es auf jedem Node vorhanden sein, deswegen empfiehlt sich die Verwendung von HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark2 = SparkSession.builder.appName(\"Test1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_path = \"file:///revenue_2021_100.csv\" #local_fs\n",
    "hdfs_path = \"hdfs://spark-jupyter-m/user/hdfs/spark_csv/revenue_2021_100.csv\" # hdfs\n",
    "df = spark2.read.csv(fs_path, header=True)\n",
    "\n",
    "type(df), df.printSchema(), df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Aggregation\n",
    "Das DF kann entweder direkt aggregiert werden, oder vorher in ein RDD umgewandelt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!less spark_csv/revenue_2021_100.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation als DF\n",
    "grouped_df = df.groupby(\"State\").sum(\"Revenue\").sort(\"Sum(Revenue)\", ascending = False)\n",
    "\n",
    "# Aggregation als RDD\n",
    "rdd = df.rdd.drop(\"ObjectID\", \"EIN\", \"Name\", \"RevenueEZ\") # U\n",
    "from operator import add\n",
    "reduced_rdd = rdd.reduceByKey(add).sortBy(lambda x: x[1], ascending = False)\n",
    "reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.cloud import storage\n",
    "#client = storage.Client()\n",
    "# https://console.cloud.google.com/storage/browser/[bucket-id]/\n",
    "#bucket = client.get_bucket('sparkbucket02')\n",
    "# Then do other things...\n",
    "#blob = bucket.get_blob(\"revenue_2021_100.csv\") #('remote/path/to/file.txt')\n",
    "\n",
    "#df = pd.read_csv(blob.download_as_string())\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession \\\n",
    "#    .builder \\\n",
    "#    .appName(\"Protob Conversion to Parquet\") \\\n",
    "#    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#    .getOrCreate()\\\n",
    "\n",
    "#df = spark.read.csv('/home/hadoop/observations_temp.csv, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo su - hdfs  \n",
    "hdfs dfsadmin -safemode leave  \n",
    "\n",
    "hdfs dfs -mkdir spark_csv  \n",
    "hdfs dfs -put /spark_csv/revenue_2021_100.csv spark_csv/revenue_2021_100.csv  \n",
    "hdfs dfs -ls spark_csv  \n",
    "\n",
    "https://stackoverflow.com/questions/42091575/pyspark-load-file-path-does-not-exist\n",
    "https://stackoverflow.com/questions/33055403/how-to-navigate-directories-in-hadoop-hdfs\n",
    "https://stackoverflow.com/questions/28213116/hadoop-copy-a-local-file-system-folder-to-hdfs\n",
    "https://stackoverflow.com/questions/61197811/can-i-read-csv-files-from-google-storage-using-spark-in-more-than-one-executor\n",
    "https://groups.google.com/g/cloud-dataproc-discuss/c/cubkWrjkk2g?pli=1\n",
    "https://stackoverflow.com/questions/56448009/storing-source-file-in-google-dataproc-hdfs-vs-google-cloud-storagegoogle-bucke\n",
    "\n",
    "hdfs dfs -put /spark_csv/revenue_2021_100.csv /user/root/spark_csv/revenue_2021_100.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
