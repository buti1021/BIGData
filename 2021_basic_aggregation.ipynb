{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df21 = pd.read_csv(\"/opt/conda/anaconda/lib/python3.6/site-packages/irsx/CSV/index_2021.csv\",index_col=False)\n",
    "object_ids = df21[\"OBJECT_ID\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ids_s = object_ids[:100]\n",
    "object_ids_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from irsx.xmlrunner import XMLRunner\n",
    "xml_runner = XMLRunner()\n",
    "list_all = []\n",
    "for e in object_ids_s:\n",
    "    filing = xml_runner.run_filing(e)\n",
    "    schedules = filing.list_schedules()\n",
    "    \n",
    "    ein = 0\n",
    "    state = 0\n",
    "    name = 0\n",
    "    revenue = 0\n",
    "    revenueEZ = 0\n",
    "    \n",
    "    if \"ReturnHeader990x\" in schedules:\n",
    "        header = filing.get_parsed_sked(\"ReturnHeader990x\")\n",
    "        #print(len(header))\n",
    "        header_part_i = header[0][\"schedule_parts\"][\"returnheader990x_part_i\"]\n",
    "        #print(header_part_i)\n",
    "        ein = header_part_i[\"ein\"]\n",
    "        try:\n",
    "            state = header_part_i[\"USAddrss_SttAbbrvtnCd\"]\n",
    "        except KeyError:\n",
    "            state = XX\n",
    "        name = header_part_i[\"BsnssNm_BsnssNmLn1Txt\"]\n",
    "        #print(ein, state, name)\n",
    "        \n",
    "    if \"IRS990EZ\" in schedules:\n",
    "        irs990ez = filing.get_parsed_sked(\"IRS990EZ\")\n",
    "        irs990ez_part_i = irs990ez[0][\"schedule_parts\"][\"ez_part_i\"]\n",
    "        revenueEZ = irs990ez_part_i[\"TtlRvnAmt\"]        \n",
    "    \n",
    "    if \"IRS990\" in schedules:\n",
    "        irs990 = filing.get_parsed_sked(\"IRS990\")\n",
    "        #print(len(irs990))\n",
    "        irs990_part_i = irs990[0][\"schedule_parts\"][\"part_i\"]\n",
    "        revenue = irs990_part_i[\"CYTtlRvnAmt\"]\n",
    "        #print(revenue)\n",
    "        \n",
    "    list = [e, ein, state, name, revenue, revenueEZ]\n",
    "    list_all.append(list)\n",
    "    \n",
    "df_result = pd.DataFrame(list_all, columns =['ObjectID', 'EIN', 'State', 'Name', 'Revenue', 'RevenueEZ'])\n",
    "df_result.to_csv('revenue_2021_100.csv', index = False)\n",
    "df_result\n",
    "  \n",
    "\n",
    "        \n",
    "    #print(e, ein, state, name, revenue, revenueEZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a Sparkession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext(appName=\"BigDataIRS3\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen des Files  \n",
    "Das File kann entweder lokal oder mittels hdfs eingelesen werden  \n",
    "Falls das File lokal eingelesen wird muss es auf jedem Node vorhanden sein, deswegen empfiehlt sich die Verwendung von HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark2 = SparkSession.builder.appName(\"Test1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_path = \"file:///revenue_2021_100.csv\" #local_fs\n",
    "hdfs_path = \"hdfs://spark-jupyter-m/user/hdfs/spark_csv/revenue_2021_100.csv\" # hdfs\n",
    "df = spark2.read.csv(fs_path, header=True)\n",
    "\n",
    "type(df), df.printSchema(), df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Aggregation\n",
    "Das DF kann entweder direkt aggregiert werden, oder vorher in ein RDD umgewandelt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!less spark_csv/revenue_2021_100.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation als DF\n",
    "grouped_df = df.groupby(\"State\").sum(\"Revenue\").sort(\"Sum(Revenue)\", ascending = False)\n",
    "\n",
    "# Aggregation als RDD\n",
    "rdd = df.rdd.drop(\"ObjectID\", \"EIN\", \"Name\", \"RevenueEZ\") # U\n",
    "from operator import add\n",
    "reduced_rdd = rdd.reduceByKey(add).sortBy(lambda x: x[1], ascending = False)\n",
    "reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.cloud import storage\n",
    "#client = storage.Client()\n",
    "# https://console.cloud.google.com/storage/browser/[bucket-id]/\n",
    "#bucket = client.get_bucket('sparkbucket02')\n",
    "# Then do other things...\n",
    "#blob = bucket.get_blob(\"revenue_2021_100.csv\") #('remote/path/to/file.txt')\n",
    "\n",
    "#df = pd.read_csv(blob.download_as_string())\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession \\\n",
    "#    .builder \\\n",
    "#    .appName(\"Protob Conversion to Parquet\") \\\n",
    "#    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#    .getOrCreate()\\\n",
    "\n",
    "#df = spark.read.csv('/home/hadoop/observations_temp.csv, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo su - hdfs  \n",
    "hdfs dfsadmin -safemode leave  \n",
    "\n",
    "hdfs dfs -mkdir spark_csv  \n",
    "hdfs dfs -put /spark_csv/revenue_2021_100.csv spark_csv/revenue_2021_100.csv  \n",
    "hdfs dfs -ls spark_csv  \n",
    "\n",
    "https://stackoverflow.com/questions/42091575/pyspark-load-file-path-does-not-exist\n",
    "https://stackoverflow.com/questions/33055403/how-to-navigate-directories-in-hadoop-hdfs\n",
    "https://stackoverflow.com/questions/28213116/hadoop-copy-a-local-file-system-folder-to-hdfs\n",
    "https://stackoverflow.com/questions/61197811/can-i-read-csv-files-from-google-storage-using-spark-in-more-than-one-executor\n",
    "https://groups.google.com/g/cloud-dataproc-discuss/c/cubkWrjkk2g?pli=1\n",
    "https://stackoverflow.com/questions/56448009/storing-source-file-in-google-dataproc-hdfs-vs-google-cloud-storagegoogle-bucke\n",
    "\n",
    "hdfs dfs -put /spark_csv/revenue_2021_100.csv /user/root/spark_csv/revenue_2021_100.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
