{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting index file for year: 2021 remote=https://s3.amazonaws.com/irs-form-990/index_2021.csv local=/opt/conda/miniconda3/lib/python3.8/site-packages/irsx/CSV/index_2021.csv\n",
      "Beginning streaming download of https://s3.amazonaws.com/irs-form-990/index_2021.csv\n",
      "Total file size: 55.76 MB\n",
      "Download completed to /opt/conda/miniconda3/lib/python3.8/site-packages/irsx/CSV/index_2021.csv in 0:00:07.584222\n"
     ]
    }
   ],
   "source": [
    "#Download Index file\n",
    "!irsx_index --year=2021 --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 422 µs (started: 2022-01-25 12:34:33 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# To have time runtime for cells\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.83 s (started: 2022-01-25 12:34:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "path = \"/opt/conda/miniconda3/lib/python3.8/site-packages/irsx/CSV/index_2021.csv\"\n",
    "\n",
    "df21 = pd.read_csv(path, index_col=False, dtype=str) # read all as string, not beautiful but we only need object id anyways\n",
    "df21.head()\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.22 s (started: 2022-01-25 12:35:13 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "sdf = spark.createDataFrame(df21[\"OBJECT_ID\"], StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(z)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 518 ms (started: 2022-01-25 12:35:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from irsx.xmlrunner import XMLRunner\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "xml_runner = XMLRunner()\n",
    "def transform_data(e):\n",
    "    try:\n",
    "        filing = xml_runner.run_filing(e)\n",
    "        schedules = filing.list_schedules()\n",
    "    except:\n",
    "        print(f\"Transform error for id {e}\")\n",
    "        return [\"\",\"\",\"\",\"\",0]\n",
    "    \n",
    "    ein = 0\n",
    "    state = 0\n",
    "    name = 0\n",
    "    revenue = 0\n",
    "    revenueEZ = 0\n",
    "    \n",
    "    if \"ReturnHeader990x\" in schedules:\n",
    "        header = filing.get_parsed_sked(\"ReturnHeader990x\")\n",
    "        header_part_i = header[0][\"schedule_parts\"][\"returnheader990x_part_i\"]\n",
    "        ein = header_part_i[\"ein\"]\n",
    "        state = header_part_i.get(\"USAddrss_SttAbbrvtnCd\", \"XX\")\n",
    "        name = header_part_i[\"BsnssNm_BsnssNmLn1Txt\"]\n",
    "        \n",
    "    if \"IRS990EZ\" in schedules:\n",
    "        irs990ez = filing.get_parsed_sked(\"IRS990EZ\")\n",
    "        irs990ez_part_i = irs990ez[0][\"schedule_parts\"].get(\"ez_part_i\", None)\n",
    "        if irs990ez_part_i:\n",
    "            revenueEZ = irs990ez_part_i.get(\"TtlRvnAmt\", 0)        \n",
    "    \n",
    "    if \"IRS990\" in schedules:\n",
    "        irs990 = filing.get_parsed_sked(\"IRS990\")\n",
    "        irs990_part_i = irs990[0][\"schedule_parts\"][\"part_i\"]\n",
    "        revenue = irs990_part_i[\"CYTtlRvnAmt\"]\n",
    "    \n",
    "    revenue = int(revenue) + int(revenueEZ)\n",
    "    return [e, ein, state, name, revenue]\n",
    "     \n",
    "    \n",
    "my_schema = StructType([\n",
    "    StructField(\"ObjectID\", StringType(), nullable=False),\n",
    "    StructField(\"EIN\", StringType(), nullable=False),\n",
    "    StructField(\"State\", StringType(), nullable=False),\n",
    "    StructField(\"Name\", StringType(), nullable=False),\n",
    "    StructField(\"Revenue\", IntegerType(), nullable=False),\n",
    "])\n",
    "\n",
    "spark_transform_data = udf(lambda z: transform_data(z), my_schema)\n",
    "spark.udf.register(\"spark_transform_data\", spark_transform_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/25 12:37:15 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6 contains a task of very large size (5208 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184781\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange RoundRobinPartitioning(10), REPARTITION_WITH_NUM, [id=#154]\n",
      "   +- Sample 0.0, 0.4, false, 43\n",
      "      +- Scan ExistingRDD[value#0]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/25 12:37:17 WARN org.apache.spark.scheduler.TaskSetManager: Stage 12 contains a task of very large size (5208 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1h 14min 7s (started: 2022-01-25 12:37:15 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "small_sdf = sdf.sample(0.4, seed=43).repartition(10)\n",
    "anz = small_sdf.count()\n",
    "print(anz)\n",
    "small_sdf2 = small_sdf.withColumn('valuelist', spark_transform_data('value')).select(\"valuelist.*\")\n",
    "#small_sdf2.show()\n",
    "small_sdf.explain()\n",
    "small_sdf2.toPandas().to_csv(f\"BIGData/{anz}.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/25 14:49:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 18 contains a task of very large size (5208 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461887\n",
      "== Physical Plan ==\n",
      "*(2) Project [pythonUDF0#69.ObjectID AS ObjectID#59, pythonUDF0#69.EIN AS EIN#60, pythonUDF0#69.State AS State#61, pythonUDF0#69.Name AS Name#62, pythonUDF0#69.Revenue AS Revenue#63]\n",
      "+- BatchEvalPython [<lambda>(value#0)], [pythonUDF0#69]\n",
      "   +- *(1) Scan ExistingRDD[value#0]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/25 14:49:51 WARN org.apache.spark.scheduler.TaskSetManager: Stage 21 contains a task of very large size (5208 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 21:=============================>                            (1 + 1) / 2]\r"
     ]
    }
   ],
   "source": [
    "# full file\n",
    "anz = sdf.count()\n",
    "print(anz)\n",
    "sdf2 = sdf.withColumn('valuelist', spark_transform_data('value')).select(\"valuelist.*\")\n",
    "sdf2.explain()\n",
    "sdf2.toPandas().to_csv(f\"hdfs://big-spark-cluster-m/user/root/{anz}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeiten von erfolgreichen Läufen\n",
    "\n",
    "|Anzahl | Zeit| Kommentar |  \n",
    "-------|--------|---------------\n",
    "|4710 | time: 2min 44s (started: 2022-01-17 18:46:15 +00:00) | erster Versuch|\n",
    "|46099| time: 12min 22s (started: 2022-01-17 20:42:31 +00:00) | erster Versuch mit ErrorHandling|\n",
    "|184781| time: 1h 14min 7s (started: 2022-01-25 12:37:15 +00:00) | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a Sparkession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen des Files  \n",
    "Das File kann entweder lokal oder mittels hdfs eingelesen werden  \n",
    "Falls das File lokal eingelesen wird muss es auf jedem Node vorhanden sein, deswegen empfiehlt sich die Verwendung von HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark2 = SparkSession.builder.appName(\"Test1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- ObjectID: long (nullable = true)\n",
      " |-- EIN: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Revenue: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(pyspark.sql.dataframe.DataFrame,\n",
       " None,\n",
       " [Row(_c0=0, ObjectID=202023169349201962, EIN=113463183, State='NY', Name='77th PRECINT COMMUNITY COUNCIL INC', Revenue=3800),\n",
       "  Row(_c0=1, ObjectID=202013049349200436, EIN=202347170, State='MD', Name='SOUTHERN MARYLAND FAST PITCH ORGANIZATION', Revenue=25259),\n",
       "  Row(_c0=2, ObjectID=202120609349300112, EIN=475378165, State='CA', Name='OAKLAND WINE FESTIVAL AND FOUNDATION', Revenue=225),\n",
       "  Row(_c0=3, ObjectID=202100489349300620, EIN=582003159, State='AR', Name='TOTAL LIFE COMMUNITY EDUC FOUNDATION', Revenue=363307),\n",
       "  Row(_c0=4, ObjectID=202023189349306407, EIN=760536563, State='CT', Name='BOZRAH INTERNATIONAL MINISTRIES INC', Revenue=927063)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_path = \"file:///revenue_2021_100.csv\" #local_fs\n",
    "hdfs_path = \"hdfs://big-spark-cluster-m/user/root/46099.csv\" # hdfs\n",
    "df = spark2.read.csv(hdfs_path, header=True, inferSchema=True)\n",
    "\n",
    "type(df), df.printSchema(), df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Aggregation\n",
    "Das DF kann entweder direkt aggregiert werden, oder vorher in ein RDD umgewandelt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",ObjectID,EIN,State,Name,Revenue\n",
      "0,202023169349201962,113463183,NY,77th PRECINT COMMUNITY COUNCIL INC,3800\n",
      "1,202013049349200436,202347170,MD,SOUTHERN MARYLAND FAST PITCH ORGANIZATION,25259\n",
      "2,202120609349300112,475378165,CA,OAKLAND WINE FESTIVAL AND FOUNDATION,225\n",
      "3,202100489349300620,582003159,AR,TOTAL LIFE COMMUNITY EDUC FOUNDATION,363307\n",
      "4,202023189349306407,760536563,CT,BOZRAH INTERNATIONAL MINISTRIES INC,927063\n",
      "5,202043219349317174,822282576,OH,ONECITY FOR RECOVERY INC,51184\n",
      "6,202013219349211076,470873877,LA,Grace Place Ministries Inc,164849\n",
      "7,202110539349200126,870420899,UT,UTAH ACADEMY OF GENERAL DENTISTRY,12617\n",
      "8,202033229349300123,450255772,ND,UNITED WAY OF GRAND FORKS EAST GRAND,565469\n"
     ]
    }
   ],
   "source": [
    "!head BIGData/46099.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Revenue: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('NY', 26282974544),\n",
       " ('CA', 22599757286),\n",
       " ('PA', 14021197075),\n",
       " ('MO', 9767605990),\n",
       " ('TX', 8408306891),\n",
       " ('FL', 6441797903),\n",
       " ('OH', 6364770882),\n",
       " ('TN', 6352786890),\n",
       " ('GA', 6155584013),\n",
       " ('MD', 6043965992),\n",
       " ('VA', 5411817304),\n",
       " ('NC', 5311445382),\n",
       " ('WA', 4838719241),\n",
       " ('IL', 4763859142),\n",
       " ('MN', 4430065258),\n",
       " ('IN', 4276565192),\n",
       " ('NJ', 3866619086),\n",
       " ('MA', 3838411645),\n",
       " ('DC', 3286803190),\n",
       " ('MI', 3224375440),\n",
       " ('KY', 3165665585),\n",
       " ('IA', 2725559896),\n",
       " ('NH', 2717014995),\n",
       " ('WI', 2610280298),\n",
       " ('AZ', 2317167743),\n",
       " ('OR', 2296702694),\n",
       " ('SD', 2251631584),\n",
       " ('NE', 2024525833),\n",
       " ('CO', 1986296862),\n",
       " ('ME', 1911403229),\n",
       " ('CT', 1671984299),\n",
       " ('LA', 1642874943),\n",
       " ('DE', 1548520157),\n",
       " ('MT', 1419515866),\n",
       " ('OK', 1255335798),\n",
       " ('XX', 1113835035),\n",
       " ('ID', 1033109314),\n",
       " ('SC', 985934691),\n",
       " ('KS', 956909342),\n",
       " ('AL', 816325971),\n",
       " ('NV', 575109238),\n",
       " ('RI', 569357888),\n",
       " ('VT', 518079533),\n",
       " ('UT', 499304361),\n",
       " ('ND', 488519360),\n",
       " ('WY', 453860239),\n",
       " ('MS', 438280172),\n",
       " ('NM', 434322177),\n",
       " ('WV', 371067524),\n",
       " ('HI', 369879970),\n",
       " ('AK', 221611971),\n",
       " ('AR', 214148178),\n",
       " ('VI', 103234800),\n",
       " ('PR', 22377120),\n",
       " ('AE', 1911788),\n",
       " ('MP', 283051)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregation als RDD\n",
    "two_col_df = df.drop(\"ObjectID\", \"EIN\", \"Name\", \"RevenueEZ\", \"_c0\") #col _c0 may (not) exist so drop or not drop it\n",
    "two_col_df.printSchema()\n",
    "rdd = two_col_df.rdd\n",
    "\n",
    "from operator import add\n",
    "reduced_rdd = rdd.reduceByKey(add).sortBy(lambda x: x[1], ascending = False)\n",
    "reduced_rdd.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
